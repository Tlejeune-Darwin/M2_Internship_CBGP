---
title: "Sim_ABCRF"
author: "LEJEUNE Thomas"
date: "`r Sys.Date()`"
output: html_document
runtime: shiny
---

```{r = Package libraries}

library(tidyverse)
library(abcrf)
library(ggplot2)
library(tinytex)
library(dplyr)
library(openxlsx)

```

```{r = Set working directory}
# Desktop detection
home_path <- Sys.getenv("HOME")

desktop_path <- if (dir.exists(file.path(home_path, "Desktop"))) {
  file.path(home_path, "Desktop")
} else if (dir.exists(file.path(home_path, "Bureau"))) {
  file.path(home_path, "Bureau")
} else {
  stop
}

# Complete path
file_path_data <- file.path("C:/Users/poupe/simulations/data_model.csv")

# Reading file
data <- read_csv(file_path_data)

```

```{r = Hard cleaning (Column removal - Automatic version)}

# Function that will search every column and count the "NA", then remove the columns having too much missing values according to the threshold
remove_high_na_cols <- function(data, threshold = 0.1) {
  total_rows <- nrow(data)
  na_summary <- data %>%
    summarise(across(everything(), ~ sum(is.na(.)))) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "NA_count") %>%
    mutate(
      Total_rows = total_rows,
      NA_percentage = round((NA_count / Total_rows) * 100, 2)
    )
  cols_to_remove <- na_summary %>%
    filter(NA_percentage > threshold * 100) %>%
    pull(Variable)
  data_clean <- data %>% select(-any_of(cols_to_remove))
  list(data_clean = data_clean, removed = cols_to_remove, na_summary = na_summary)
}
result <- remove_high_na_cols(data, threshold = 0.05)
data_clean <- result$data 
result$na_summary  # List of removed col

top_n_display <- 20

ggplot(result$na_summary %>% 
         filter(NA_percentage > 0) %>% 
         slice_max(NA_percentage, n = top_n_display),
       aes(x = reorder(Variable, -NA_percentage), y = NA_percentage)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 30 variables with most NA",
    x = "Resuming stats",
    y = "NA percentage (%)"
  ) +
  theme_minimal(base_size = 14)

```

```{r = Delete all columns}
# Merge all th columns to delete
cols_to_remove <- c(
  # Ne stats
  "P_Ne_0.050", "P_Ne_0.020", "P_Ne_0.010", "P_Ne_0.000",
  "N_Ne_0.050", "N_Ne_0.020", "N_Ne_0.010", "N_Ne_0.000",
  "J_Ne_0.050", "J_Ne_0.020", "J_Ne_0.010", "J_Ne_0.000",
  
  #"mean_exp_het_pop2", "mean_exp_het_pop1",
  #"var_alleles_pop2", "var_alleles_pop1", 
  #"var_allele_size_pop2", "var_allele_size_pop1",
  #"sum_alleles_pop1", "sum_alleles_pop2",
  #"mean_obs_het_pop1", "mean_obs_het_pop2",
  #"mean_alleles_pop1", "mean_alleles_pop2",
  
  # census_N & batch
  "census_N_1", "census_N_2", "census_N_3", "census_N_4", 
  "census_N_5", "census_N_6", "census_N_7", "census_N_8",
  "census_N_9", "census_N_10", "census_N_11",
  "batch", "simulation_id")

# Remove the lines with NA
data_clean <- data_clean %>% 
  select(-any_of(cols_to_remove)) %>%
  na.omit()

# Add random columns with values between 0 and 100
data_clean <- data_clean %>%
  mutate(
    random_1 = runif(n(), min = 0, max = 100),
    random_2 = runif(n(), min = 0, max = 100),
    random_3 = runif(n(), min = 0, max = 100)
  )
```

```{r = MatchCount mean and variance calculations }

## {r = MatchCount mean, variance, median & CV calculations }

# 1. Récupère toutes les colonnes MatchCount
matchcount_cols <- grep("^MatchCount", names(data_clean), value = TRUE)

# 2. Calcule la moyenne et la variance
data_clean$MatchCount_mean <- rowMeans(data_clean[, matchcount_cols], na.rm = TRUE)
data_clean$MatchCount_var  <- apply(data_clean[, matchcount_cols],  1, var,    na.rm = TRUE)

# 3. Calcule la médiane
data_clean$MatchCount_median <- apply(data_clean[, matchcount_cols], 1, median, na.rm = TRUE)

# 4. Calcule l’écart-type et le coefficient de variation (CV = SD / mean)
data_clean$MatchCount_sd <- apply(data_clean[, matchcount_cols], 1, sd, na.rm = TRUE)
data_clean$MatchCount_cv <- data_clean$MatchCount_sd / data_clean$MatchCount_mean

# 5. Conserve toutes les autres colonnes (en supprimant les colonnes "brutes" MatchCount_*)
other_cols   <- setdiff(names(data_clean), matchcount_cols)
data_clean   <- data_clean[, other_cols]

```

```{r = Harmonic He mean}
# 1. Extract all the Realized_He columns
ne_cols <- grep("^Realized_Ne_", names(data_clean), value = TRUE)

# 2. Calculate the Harmonic mean
harmonic_mean <- function(x) {
  x <- as.numeric(x)
  x <- x[!is.na(x) & x > 0]
  if (length(x) == 0) return(NA)
  return(length(x) / sum(1 / x))
}

# Apply the function to each line
data_clean$Harmonic_Ne <- apply(data_clean[, ne_cols], 1, harmonic_mean)

# 3. Calculate the Ne / N ratio
if ("pop_size" %in% colnames(data_clean)) {
  data_clean$Ne_N_ratio <- data_clean$Harmonic_Ne / data_clean$pop_size
} else {
  warning("missing pop_size column : no ratio calculated")
}

```

```{r}
# 5. Prélèvement de 20 000 lignes aléatoires
n_total <- nrow(data_clean)
n_sample <- min(20000, n_total)
sample_indices <- sample(seq_len(n_total), size = n_sample, replace = FALSE)

data_clean <- data_clean[sample_indices, ]

```

```{r = Set the parameters and resuming statistics tables}

# This part allows to make a overall dataframe with all the informative resuming statistics
# List of parameters
param_cols <- c("pop_size", "num_loci", "sample1_size_Ne", "sample2_size_Ne",
                "sample_size_CMR", "mutation_rate", "recap_Ne", "Harmonic_Ne", "Ne_N_ratio")
params <- data_clean %>% select(all_of(param_cols))

# List of resuming statistics
stat_keywords <- c("id", "LD", "HE", "Coan", "het", "alleles", "P_", "N_F", "J_", "MatchCount_", "random")
stat_cols <- names(data_clean)[sapply(names(data_clean), function(col) any(str_detect(col, stat_keywords)))]
stats_table <- data_clean %>% select(all_of(stat_cols))

```

```{r = Stats table : CMR}

# This part is useful for the study of N estimations, only ecological statistics are represented
param_cols <- c("pop_size", "num_loci", "sample1_size_Ne", "sample2_size_Ne",
                "sample_size_CMR", "mutation_rate", "recap_Ne", "Harmonic_Ne", "Ne_N_ratio")
params_N <- data_clean %>% select(all_of(param_cols))

stat_keywords <- c("id", "MatchCount_", "random")
stat_cols <- names(data_clean)[sapply(names(data_clean), function(col) any(str_detect(col, stat_keywords)))]
stats_table_N <- data_clean %>% select(all_of(stat_cols))
```

```{r = Stats table : Genetic}

# This part is useful for the study of Ne estimations, only genetic statistics are represented
param_cols <- c("pop_size", "num_loci", "sample1_size_Ne", "sample2_size_Ne",
                "sample_size_CMR", "mutation_rate", "recap_Ne", "Harmonic_Ne", "Ne_N_ratio")
params_Ne <- data_clean %>% select(all_of(param_cols))

stat_keywords <- c("id", "LD", "HE", "Coan", "het", "alleles", "P_", "N_F", "J_", "random")
stat_cols <- names(data_clean)[sapply(names(data_clean), function(col) any(str_detect(col, stat_keywords)))]
stats_table_Ne <- data_clean %>% select(all_of(stat_cols))
```

```{r = ABCRF model - pop_size - all data}

# Use "pop-size" as the target parameter. It will be used by the ABC-RF model to choose which resuming statistics are the most useful to predict N values.
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_N <- "pop_size"
learning_data_N_all <- bind_cols(y = params[[target_param_N]], stats_table) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_all <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_all, ntree = 50)
summary(model_rf_N_all)

```

```{r = ABCRF model - pop_size - CMR data}
# Use the specific data ("MatchCount") to get N predictions and construct the model
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_N <- "pop_size"
learning_data_N_CMR <- bind_cols(y = params_N[[target_param_N]], stats_table_N) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_CMR <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_CMR, ntree = 50)
summary(model_rf_N_CMR)

```

```{r = ABCRF model - pop_size - genetic data}
# Use the specific data ("MatchCount") to get N predictions and construct the model
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_N <- "pop_size"
learning_data_N_gen<- bind_cols(y = params_Ne[[target_param_N]], stats_table_Ne) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_gen <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_gen, ntree = 50)
summary(model_rf_N_gen)

```

```{r = ABCRF model - Realized_Ne - all data}

# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_Ne <- "Harmonic_Ne"
learning_data_Ne_all <- bind_cols(y = params[[target_param_Ne]], stats_table) %>% rename(!!target_param_Ne := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_Ne_all <- regAbcrf(as.formula(paste(target_param_Ne, "~ .")), data = learning_data_Ne_all, ntree = 50)
summary(model_rf_Ne_all)

```

```{r = ABCRF model - Realized_Ne - genetic data}

# Use "Harmonic_Ne" as the target parameter. It will be used by the ABC-RF model to choose which resuming statistics are the most useful to predict Ne values.
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_Ne <- "Harmonic_Ne"
learning_data_Ne_gen <- bind_cols(y = params_Ne[[target_param_Ne]], stats_table_Ne) %>% rename(!!target_param_Ne := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_Ne_gen <- regAbcrf(as.formula(paste(target_param_Ne, "~ .")), data = learning_data_Ne_gen, ntree = 50)
summary(model_rf_Ne_gen)

```

```{r = ABCRF model - Realized_Ne - CMR data}

# Use "Harmonic_Ne" as the target parameter. It will be used by the ABC-RF model to choose which resuming statistics are the most useful to predict Ne values.
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_Ne <- "Harmonic_Ne"
learning_data_Ne_CMR <- bind_cols(y = params_N[[target_param_Ne]], stats_table_N) %>% rename(!!target_param_Ne := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_Ne_CMR <- regAbcrf(as.formula(paste(target_param_Ne, "~ .")), data = learning_data_Ne_CMR, ntree = 50)
summary(model_rf_Ne_CMR)

```

```{r = ABCRF model - Ne_N_ratio - all data}

# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_Ne_N <- "Ne_N_ratio"
learning_data_Ne_N <- bind_cols(y = params[[target_param_Ne_N]], stats_table) %>% rename(!!target_param_Ne_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_Ne_N <- regAbcrf(as.formula(paste(target_param_Ne_N, "~ .")), data = learning_data_Ne_N, ntree = 100)
summary(model_rf_Ne_N)

```

```{r = Stat importance for ABCRF model - pop_size - all_data}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_N_all <- model_rf_N_all$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# Only keep the 20 most useful variable 
top_n <- 20
# Create a variable of colour on the "simulation_id". Its the variable that shows every other variable non pertinent to the model. Every variables that are below this one are significantly unuseful.
# Créer une variable de couleur
importance_top_N_all <- importance_df_N_all %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median", "MatchCount_cv", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_N_all, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - All_data - N",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r = Stat importance for ABCRF model - pop_size - CMR}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_N_CMR <- model_rf_N_CMR$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# Only keep the 20 most useful variable 
top_n <- 20
# Create a variable of colour on the "simulation_id". Its the variable that shows every other variable non pertinent to the model. Every variables that are below this one are significantly unuseful.
# Créer une variable de couleur
importance_top_N_CMR <- importance_df_N_CMR %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_N_CMR, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - CMR_only - Ne",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r = Stat importance for ABCRF model - pop_size - genetic}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_N_gen <- model_rf_N_gen$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# Only keep the 20 most useful variable 
top_n <- 20
# Create a variable of colour on the "simulation_id". Its the variable that shows every other variable non pertinent to the model. Every variables that are below this one are significantly unuseful.
# Créer une variable de couleur
importance_top_N_gen <- importance_df_N_gen %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_N_gen, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - Genetic_only - N",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r = ABCRF model - Ne_N_ratio - all data}

# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_Ne_N <- "Ne_N_ratio"
learning_data_Ne_N <- bind_cols(y = params[[target_param_Ne_N]], stats_table) %>% rename(!!target_param_Ne_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_Ne_N <- regAbcrf(as.formula(paste(target_param_Ne_N, "~ .")), data = learning_data_Ne_N, ntree = 100)
summary(model_rf_Ne_N)

```

```{r = Stat importance for ABCRF model - Harmonic Ne - all_data}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_Ne_all <- model_rf_Ne_all$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# 2. Ne garder que les 20 premières variables
top_n <- 20
# Créer une variable de couleur
# Créer une variable de couleur
importance_top_Ne_all <- importance_df_Ne_all %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median","MatchCount_cv", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_Ne_all, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - all_data - Ne",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
```

```{r = Stat importance for ABCRF model - Harmonic Ne - genetic}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_Ne_gen <- model_rf_Ne_gen$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# 2. Ne garder que les 20 premières variables
top_n <- 20

importance_top_Ne_gen <- importance_df_N_gen %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_Ne_gen, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - Genetic_only - Ne",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r = Stat importance for ABCRF model - Harmonic Ne - CMR}

# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_Ne_CMR <- model_rf_Ne_CMR$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# 2. Ne garder que les 20 premières variables
top_n <- 20
# Créer une variable de couleur
# Créer une variable de couleur
importance_top_Ne_CMR <- importance_df_Ne_CMR %>%
  slice_max(order_by = Importance, n = top_n) %>%
  mutate(color = case_when(
    Statistic %in% c("random_1", "random_2", "random_3") ~ "highlight",
    Statistic %in% c("MatchCount_var", "MatchCount_mean", "MatchCount_median", "MatchCount_sd") ~ "match",
    TRUE ~ "normal"
  )) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance))

# Graphic 
ggplot(importance_top_Ne_CMR, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c(
    "highlight" = "red",
    "match" = "darkgreen",
    "normal" = "steelblue"
  )) +
  labs(
    title = "Important resuming stats - CMR_only - Ne",
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r = Stat importance for ABCRF model - Ne_N_ratio}
# Determines the order of importance of every resuming stat in the dataframe concerning the prediction of the parameter
importance_df_Ne_N <- model_rf_Ne_N$model.rf$variable.importance %>%
  sort(decreasing = TRUE) %>%
  enframe(name = "Statistic", value = "Importance")

# Only keep the 20 most useful variable 
top_n <- 20
# Create a variable of colour on the "simulation_id". Its the variable that shows every other variable non pertinent to the model. Every variables that are below this one are significantly unuseful.
importance_top_Ne_N <- importance_df_Ne_N %>%
  mutate(color = ifelse(Statistic == "simulation_id", "highlight", "normal")) %>%
  mutate(Statistic = fct_reorder(Statistic, Importance)) %>%
  slice_max(order_by = Importance, n = top_n)

importance_top_Ne_N <- importance_top_Ne_N %>%
  mutate(color = ifelse(Statistic == "simulation_id", "highlight", "normal"))

# Graphique
ggplot(importance_top_Ne_N, aes(x = Statistic, y = Importance, fill = color)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(values = c("highlight" = "red", "normal" = "steelblue")) +
  labs(
    title = paste("Top", top_n, "most important variables for", target_param_Ne_N),
    x = "Statistics",
    y = "Importance (abcrf)"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

```

```{r prediction vs true value - pop_size - all_data}

# Création du dataframe avec vraies valeurs et prédictions
prediction_df_N_all <- data.frame(
  pop_size = learning_data_N_all$pop_size,
  prediction = model_rf_N_all$model.rf$predictions
)

res_N_all <- predictOOB(model_rf_N_all, training = learning_data_N_all)

out_N_all <- data.frame(
  random_1 = learning_data_N_all$random_1,
  true_N_all = learning_data_N_all$pop_size,
  pred_N_all = res_N_all$expectation
)

# Calcul de l'erreur relative
out_N_all$rel_error <- abs(out_N_all$pred_N_all - out_N_all$true_N_all) / out_N_all$true_N_all

# Marquage des outliers : les 0.5% les plus éloignés
percentile_cutoff <- 0.999  # 0.5% les plus extrêmes
threshold_error <- quantile(out_N_all$rel_error, percentile_cutoff, na.rm = TRUE)
out_N_all$is_outlier <- out_N_all$rel_error > threshold_error

# Supprimer les bornes (plus utiles ici)
out_N_all$lower_bound <- NA
out_N_all$upper_bound <- NA

# Fusionner les stats résumantes
stats_only <- learning_data_N_all %>% select(-pop_size)
out_N_all <- left_join(out_N_all, stats_only, by = "random_1")

# Affichage graphique
ggplot(out_N_all, aes(x = true_N_all, y = pred_N_all)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(title = paste0("All_data_N – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
       x = "True_value (N)",
       y = "Prediction",
       color = "Outlier ?") +
  theme_minimal()


# Moyennes par groupe
out_stats_N_all <- out_N_all %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_N_all <- out_N_all %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_N_all <- t(out_stats_N_all - non_out_stats_N_all) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_N_all$estimation_type <- case_when(
  out_N_all$is_outlier & out_N_all$pred_N_all > out_N_all$true_N_all ~ "surestimation",
  out_N_all$is_outlier & out_N_all$pred_N_all < out_N_all$true_N_all ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_N_all, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)


# Visualisation
ggplot(head(diffs_N_all, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_N_all <- learning_data_N_all$pop_size

# Prédictions
predicted_N_all <- res_N_all$expectation

# RMSE
rmse_N_all <- sqrt(res_N_all$MSE)

# Biais
bias_N_all <- mean(predicted_N_all - true_values_N_all)

# R²
r2_N_all <- 1 - sum((true_values_N_all - predicted_N_all)^2) / sum((true_values_N_all - mean(true_values_N_all))^2)

# RMSE_relative
rmse_re_N_all <- mean((predicted_N_all - true_values_N_all)^2 / true_values_N_all^2)
sqrt(rmse_N_all)

# Biais_relatif
bias_re_N_all <- mean((predicted_N_all - true_values_N_all) / true_values_N_all)

coverage_N_all <- res_N_all$coverage

cat("N_All_data", "\n")
cat("Coverage    : ", coverage_N_all, "\n")
cat("RMSE        : ", rmse_N_all, "\n")
cat("Bias        : ", bias_N_all, "\n")
cat("R²          : ", r2_N_all, "\n")
cat("RMSE (rel)  : ", rmse_re_N_all, "\n")
cat("Bias (rel)  : ", bias_re_N_all, "\n")
```

```{r prediction vs true value - pop_size - CMR}

# Créer un dataframe avec les vraies valeurs et les prédictions
prediction_df_N_CMR <- data.frame(
  pop_size = learning_data_N_CMR$pop_size,
  prediction = model_rf_N_CMR$model.rf$predictions
)

res_N_CMR <- predictOOB(model_rf_N_CMR, training = learning_data_N_CMR)

out_N_CMR <- data.frame(
  random_1 = learning_data_N_CMR$random_1,
  true_N_CMR = learning_data_N_CMR$pop_size,
  pred_N_CMR = res_N_CMR$expectation
)

# Erreur relative
out_N_CMR$rel_error <- abs(out_N_CMR$pred_N_CMR - out_N_CMR$true_N_CMR) / out_N_CMR$true_N_CMR

# Seuil quantile local (0.5% des pires erreurs relatives)
percentile_cutoff <- 0.999
threshold_error_CMR <- quantile(out_N_CMR$rel_error, percentile_cutoff, na.rm = TRUE)

# Marquage des outliers
out_N_CMR$is_outlier <- out_N_CMR$rel_error > threshold_error_CMR

# (On supprime les bornes inutiles)
out_N_CMR$lower_bound <- NA
out_N_CMR$upper_bound <- NA

# Extraire juste les stats résumantes
stats_only <- learning_data_N_CMR %>%
  select(-pop_size)

# Fusion par random_1
out_N_CMR <- left_join(out_N_CMR, stats_only, by = "random_1")

# Affichage graphique
ggplot(out_N_CMR, aes(x = true_N_CMR, y = pred_N_CMR)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(
    title = paste0("CMR_only_N – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
    x = "True_value (N)",
    y = "Prediction",
    color = "Outlier ?"
  ) +
  theme_minimal()

# Moyennes par groupe
out_stats_N_CMR <- out_N_CMR %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_N_CMR <- out_N_CMR %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_N_CMR <- t(out_stats_N_CMR - non_out_stats_N_CMR) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_N_CMR$estimation_type <- case_when(
  out_N_CMR$is_outlier & out_N_CMR$pred_N_CMR > out_N_CMR$true_N_CMR ~ "surestimation",
  out_N_CMR$is_outlier & out_N_CMR$pred_N_CMR < out_N_CMR$true_N_CMR ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_N_CMR, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)

# Visualisation
ggplot(head(diffs_N_CMR, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_N_CMR <- learning_data_N_CMR$pop_size

# Prédictions
predicted_N_CMR <- res_N_CMR$expectation

# RMSE
rmse_N_CMR <- sqrt(res_N_CMR$MSE)

# Biais
bias_N_CMR <- mean(predicted_N_CMR - true_values_N_CMR)

# R²
r2_N_CMR <- 1 - sum((true_values_N_CMR - predicted_N_CMR)^2) / sum((true_values_N_CMR - mean(true_values_N_CMR))^2)

# RMSE_relative
rmse_re_N_CMR <- mean((predicted_N_CMR - true_values_N_CMR)^2 / true_values_N_CMR^2)
sqrt(rmse_N_CMR)

# Biais_relatif
bias_re_N_CMR <- mean((predicted_N_CMR - true_values_N_CMR) / true_values_N_CMR)

coverage_N_CMR <- res_N_CMR$coverage

cat("N_CMR_only", "\n")
cat("Coverage    : ", coverage_N_CMR, "\n")
cat("RMSE        : ", rmse_N_CMR, "\n")
cat("Bias        : ", bias_N_CMR, "\n")
cat("R²          : ", r2_N_CMR, "\n")
cat("RMSE (rel)  : ", rmse_re_N_CMR, "\n")
cat("Bias (rel)  : ", bias_re_N_CMR, "\n")
```

```{r prediction vs true value - pop_size - Genetic}

# Créer un dataframe avec les vraies valeurs et les prédictions
prediction_df_N_gen <- data.frame(
  pop_size = learning_data_N_gen$pop_size,
  prediction = model_rf_N_gen$model.rf$predictions
)

res_N_gen <- predictOOB(model_rf_N_gen, training = learning_data_N_gen)

out_N_gen <- data.frame(
  random_1 = learning_data_N_gen$random_1,
  true_N_gen = learning_data_N_gen$pop_size,
  pred_N_gen = res_N_gen$expectation
)

# Calcul de l’erreur relative
out_N_gen$rel_error <- abs(out_N_gen$pred_N_gen - out_N_gen$true_N_gen) / out_N_gen$true_N_gen

# Définir les outliers comme les 0.5% des erreurs relatives les plus élevées
percentile_cutoff <- 0.999
threshold_error_gen <- quantile(out_N_gen$rel_error, percentile_cutoff, na.rm = TRUE)

# Marquage des outliers
out_N_gen$is_outlier <- out_N_gen$rel_error > threshold_error_gen

# Supprimer les bornes inutiles
out_N_gen$lower_bound <- NA
out_N_gen$upper_bound <- NA

# Fusionner les stats résumantes
stats_only <- learning_data_N_gen %>%
  select(-pop_size)

out_N_gen <- left_join(out_N_gen, stats_only, by = "random_1")

# Affichage graphique
ggplot(out_N_gen, aes(x = true_N_gen, y = pred_N_gen)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(
    title = paste0("Genetic_only_N – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
    x = "True_value (N)",
    y = "Prediction",
    color = "Outlier ?"
  ) +
  theme_minimal()

# Moyennes par groupe
out_stats_N_gen <- out_N_gen %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_N_gen <- out_N_gen %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_N_gen <- t(out_stats_N_gen - non_out_stats_N_gen) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_N_gen$estimation_type <- case_when(
  out_N_gen$is_outlier & out_N_gen$pred_N_gen > out_N_gen$true_N_gen ~ "surestimation",
  out_N_gen$is_outlier & out_N_gen$pred_N_gen < out_N_gen$true_N_gen ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_N_gen, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)

# Visualisation
ggplot(head(diffs_N_gen, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_N_gen <- learning_data_N_gen$pop_size

# Prédictions
predicted_N_gen <- res_N_gen$expectation

# RMSE
rmse_N_gen <- sqrt(res_N_gen$MSE)

# Biais
bias_N_gen <- mean(predicted_N_gen - true_values_N_gen)

# R²
r2_N_gen <- 1 - sum((true_values_N_gen - predicted_N_gen)^2) / sum((true_values_N_gen - mean(true_values_N_gen))^2)

# RMSE_relative
rmse_re_N_gen <- mean((predicted_N_gen - true_values_N_gen)^2 / true_values_N_gen^2)
sqrt(rmse_N_gen)

# Biais_relatif
bias_re_N_gen <- mean((predicted_N_gen - true_values_N_gen) / true_values_N_gen)

coverage_N_gen <- res_N_gen$coverage

cat("N_Gen_only", "\n")
cat("Coverage    : ", coverage_N_gen, "\n")
cat("RMSE        : ", rmse_N_gen, "\n")
cat("Bias        : ", bias_N_gen, "\n")
cat("R²          : ", r2_N_gen, "\n")
cat("RMSE (rel)  : ", rmse_re_N_gen, "\n")
cat("Bias (rel)  : ", bias_re_N_gen, "\n")
```

```{r}

# ID outliers dans les deux modèles
outliers_all <- out_N_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_spe <- out_N_spe %>% filter(is_outlier) %>% pull(simulation_id)

# Détection des groupes
common_outliers <- intersect(outliers_all, outliers_spe)
only_all <- setdiff(outliers_all, outliers_spe)
only_spe <- setdiff(outliers_spe, outliers_all)

### 2. Extraire les données brutes ====

# 3 sous-jeux de données
stats_common <- learning_data_N_all %>%
  filter(simulation_id %in% common_outliers) %>%
  select(-simulation_id, -pop_size)

stats_only_all <- learning_data_N_all %>%
  filter(simulation_id %in% only_all) %>%
  select(-simulation_id, -pop_size)

stats_only_spe <- learning_data_N_spe %>%
  filter(simulation_id %in% only_spe) %>%
  select(-simulation_id, -pop_size)

### 3. Définir les outliers statistiques pour chaque ligne ====

# Fonction d’identification par z-score
identify_stat_outliers <- function(df) {
  df_z <- scale(df)  # calcule z-score
  outlier_flags <- abs(df_z) > 2  # seuil de z-score
  as.data.frame(outlier_flags)
}

# Appliquer aux 3 groupes
flags_common <- identify_stat_outliers(stats_common)
flags_all <- identify_stat_outliers(stats_only_all)
flags_spe <- identify_stat_outliers(stats_only_spe)

### 4. Compter combien de fois chaque stat est aberrante ====

count_outliers <- function(flags_df, groupe) {
  tibble(
    stat = colnames(flags_df),
    count = colSums(flags_df, na.rm = TRUE),
    groupe = groupe
  )
}

count_common <- count_outliers(flags_common, "common")
count_all <- count_outliers(flags_all, "all_stats")
count_spe <- count_outliers(flags_spe, "CMR_only")

# Regrouper
outlier_counts <- bind_rows(count_common, count_all, count_spe)

# Garder uniquement les stats les plus fréquentes
top_stats <- outlier_counts %>%
  group_by(stat) %>%
  summarise(total = sum(count)) %>%
  arrange(desc(total)) %>%
  slice_head(n = 5) %>%
  pull(stat)

plot_data <- outlier_counts %>%
  filter(stat %in% top_stats)

### 5. Barplot empilé ====

ggplot(plot_data, aes(x = stat, y = count, fill = groupe)) +
  geom_col(position = "stack") +
  labs(
    title = "Statistiques influentes sur les outliers par groupe",
    x = "Statistique résumante",
    y = "Nombre de simulations outliers",
    fill = "Origine des outliers"
  ) +
  scale_fill_manual(values = c("common" = "orange", "all_stats" = "skyblue", "CMR_only" = "salmon")) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

```{r prediction vs true value - Harmonic_Ne - all_data}

# Créer un dataframe avec les vraies valeurs et les prédictions
prediction_df_Ne_all <- data.frame(
  Harmonic_Ne = learning_data_Ne_all$Harmonic_Ne,
  prediction = model_rf_Ne_all$model.rf$predictions
)

res_Ne_all <- predictOOB(model_rf_Ne_all, training = learning_data_Ne_all)

out_Ne_all <- data.frame(
  random_1 = learning_data_Ne_all$random_1,
  true = learning_data_Ne_all$Harmonic_Ne,
  pred = res_Ne_all$expectation
)

# Calcul de l’erreur relative
out_Ne_all$rel_error <- abs(out_Ne_all$pred - out_Ne_all$true) / out_Ne_all$true

# Marquage des outliers : seuil basé sur les 0.5% plus grosses erreurs
percentile_cutoff <- 0.999
threshold_error_Ne <- quantile(out_Ne_all$rel_error, percentile_cutoff, na.rm = TRUE)

out_Ne_all$is_outlier <- out_Ne_all$rel_error > threshold_error_Ne

# (Pas de bornes ici, on les ignore volontairement)
out_Ne_all$lower_bound <- NA
out_Ne_all$upper_bound <- NA

# Extraire les stats résumantes
stats_only <- learning_data_Ne_all %>%
  select(-Harmonic_Ne)

# Fusion avec les stats
out_Ne_all <- left_join(out_Ne_all, stats_only, by = "random_1")

# Visualisation
ggplot(out_Ne_all, aes(x = true, y = pred)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(
    title = paste0("All_data_Ne – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
    x = "True_value (Ne)",
    y = "Prediction",
    color = "Outlier ?"
  ) +
  theme_minimal()


# Moyennes par groupe
out_stats_Ne_all <- out_Ne_all %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_Ne_all <- out_Ne_all %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_Ne_all <- t(out_stats_Ne_all - non_out_stats_Ne_all) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_Ne_all$estimation_type <- case_when(
  out_Ne_all$is_outlier & out_Ne_all$pred > out_Ne_all$true ~ "surestimation",
  out_Ne_all$is_outlier & out_Ne_all$pred < out_Ne_all$true ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_Ne_all, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)


# Visualisation
ggplot(head(diffs_Ne_all, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_Ne_all <- learning_data_Ne_all$Harmonic_Ne

# Prédictions
predicted_Ne_all <- res_Ne_all$expectation

# RMSE
rmse_Ne_all <- sqrt(res_Ne_all$MSE)

# Biais
bias_Ne_all <- mean(predicted_Ne_all - true_values_Ne_all)

# RMSE_relative
rmse_re_Ne_all <- mean((predicted_Ne_all - true_values_Ne_all)^2 / true_values_Ne_all^2)

# Biais_relatif
bias_re_Ne_all <- mean((predicted_Ne_all - true_values_Ne_all) / true_values_Ne_all)

# R²
r2_Ne_all <- 1 - sum((true_values_Ne_all - predicted_Ne_all)^2) / sum((true_values_Ne_all - mean(true_values_Ne_all))^2)

coverage_Ne_all <- res_Ne_all$coverage

cat("Ne_All_data", "\n")
cat("Coverage    : ", coverage_Ne_all, "\n")
cat("RMSE        : ", rmse_Ne_all, "\n")
cat("Bias        : ", bias_Ne_all, "\n")
cat("R²          : ", r2_Ne_all, "\n")
cat("RMSE (rel)  : ", rmse_re_Ne_all, "\n")
cat("Bias (rel)  : ", bias_re_Ne_all, "\n")

```

```{r prediction vs true value - Harmonic_Ne - Genetic}

# Créer un dataframe avec les vraies valeurs et les prédictions
prediction_df_Ne_gen <- data.frame(
  Harmonic_Ne = learning_data_Ne_gen$Harmonic_Ne,
  prediction = model_rf_Ne_gen$model.rf$predictions
)

res_Ne_gen <- predictOOB(model_rf_Ne_gen, training = learning_data_Ne_gen)

out_Ne_gen <- data.frame(
  random_1 = learning_data_Ne_gen$random_1,
  true = learning_data_Ne_gen$Harmonic_Ne,
  pred = res_Ne_gen$expectation
)

# Calcul de l’erreur relative
out_Ne_gen$rel_error <- abs(out_Ne_gen$pred - out_Ne_gen$true) / out_Ne_gen$true

# Marquage des outliers : seuil basé sur les 0.5% plus grosses erreurs
percentile_cutoff <- 0.999
threshold_error_Ne_gen <- quantile(out_Ne_gen$rel_error, percentile_cutoff, na.rm = TRUE)

out_Ne_gen$is_outlier <- out_Ne_gen$rel_error > threshold_error_Ne_gen

# Supprimer les bornes du tunnel (inutile ici)
out_Ne_gen$lower_bound <- NA
out_Ne_gen$upper_bound <- NA

# Extraire les stats résumantes
stats_only <- learning_data_Ne_gen %>%
  select(-Harmonic_Ne)

# Fusion des données
out_Ne_gen <- left_join(out_Ne_gen, stats_only, by = "random_1")

# Visualisation
ggplot(out_Ne_gen, aes(x = true, y = pred)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(
    title = paste0("Genetic_only_Ne – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
    x = "True_value (Ne)",
    y = "Prediction",
    color = "Outlier ?"
  ) +
  theme_minimal()

# Moyennes par groupe
out_stats_Ne_gen <- out_Ne_gen %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_Ne_gen <- out_Ne_gen %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_Ne_gen <- t(out_stats_Ne_gen - non_out_stats_Ne_gen) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_Ne_gen$estimation_type <- case_when(
  out_Ne_gen$is_outlier & out_Ne_gen$pred > out_Ne_gen$true ~ "surestimation",
  out_Ne_gen$is_outlier & out_Ne_gen$pred < out_Ne_gen$true ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_Ne_gen, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)


# Visualisation
ggplot(head(diffs_Ne_gen, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_Ne_gen <- learning_data_Ne_gen$Harmonic_Ne

# Prédictions
predicted_Ne_gen <- res_Ne_gen$expectation

# RMSE
rmse_Ne_gen <- sqrt(res_Ne_gen$MSE)

# Biais
bias_Ne_gen <- mean(predicted_Ne_gen - true_values_Ne_gen)

# R²
r2_Ne_gen <- 1 - sum((true_values_Ne_gen - predicted_Ne_gen)^2) / sum((true_values_Ne_gen - mean(true_values_Ne_gen))^2)

# RMSE_relative
rmse_re_Ne_gen <- mean((predicted_Ne_gen - true_values_Ne_gen)^2 / true_values_Ne_gen^2)
sqrt(rmse_Ne_gen)

# Biais_relatif
bias_re_Ne_gen <- mean((predicted_Ne_gen - true_values_Ne_gen) / true_values_Ne_gen)

coverage_Ne_gen <- res_Ne_gen$coverage

cat("Ne_Gen_only", "\n")
cat("Coverage    : ", coverage_Ne_gen, "\n")
cat("RMSE        : ", rmse_Ne_gen, "\n")
cat("Bias        : ", bias_Ne_gen, "\n")
cat("R²          : ", r2_Ne_gen, "\n")
cat("RMSE (rel)  : ", rmse_re_Ne_gen, "\n")
cat("Bias (rel)  : ", bias_re_Ne_gen, "\n")
```

```{r prediction vs true value - Harmonic_Ne - CMR}

# Créer un dataframe avec les vraies valeurs et les prédictions
prediction_df_Ne_CMR <- data.frame(
  Harmonic_Ne = learning_data_Ne_CMR$Harmonic_Ne,
  prediction = model_rf_Ne_CMR$model.rf$predictions
)

res_Ne_CMR <- predictOOB(model_rf_Ne_CMR, training = learning_data_Ne_CMR)

out_Ne_CMR <- data.frame(
  random_1 = learning_data_Ne_CMR$random_1,
  true = learning_data_Ne_CMR$Harmonic_Ne,
  pred = res_Ne_CMR$expectation
)

# Calcul de l’erreur relative
out_Ne_CMR$rel_error <- abs(out_Ne_CMR$pred - out_Ne_CMR$true) / out_Ne_CMR$true

# Marquage des outliers : seuil local sur les 0.5% plus grosses erreurs
percentile_cutoff <- 0.999
threshold_error_Ne_CMR <- quantile(out_Ne_CMR$rel_error, percentile_cutoff, na.rm = TRUE)

out_Ne_CMR$is_outlier <- out_Ne_CMR$rel_error > threshold_error_Ne_CMR

# Tunnel désactivé
out_Ne_CMR$lower_bound <- NA
out_Ne_CMR$upper_bound <- NA

# Extraire les stats résumantes
stats_only <- learning_data_Ne_CMR %>%
  select(-Harmonic_Ne)

# Fusion via random_1
out_Ne_CMR <- left_join(out_Ne_CMR, stats_only, by = "random_1")

# Visualisation
ggplot(out_Ne_CMR, aes(x = true, y = pred)) +
  geom_point(aes(color = is_outlier), alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  scale_x_log10() + scale_y_log10() +
  labs(
    title = paste0("CMR_only_Ne – Top ", round((1 - percentile_cutoff) * 100, 2), "% des pires erreurs relatives"),
    x = "True_value (Ne)",
    y = "Prediction",
    color = "Outlier ?"
  ) +
  theme_minimal()

# Moyennes par groupe
out_stats_Ne_CMR <- out_Ne_CMR %>% filter(is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))
non_out_stats_Ne_CMR <- out_Ne_CMR %>% filter(!is_outlier) %>% select(where(is.numeric)) %>% summarise(across(everything(), mean))

# Comparaison
diffs_Ne_CMR <- t(out_stats_Ne_CMR - non_out_stats_Ne_CMR) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

out_Ne_CMR$estimation_type <- case_when(
  out_Ne_CMR$is_outlier & out_Ne_CMR$pred > out_Ne_CMR$true ~ "surestimation",
  out_Ne_CMR$is_outlier & out_Ne_CMR$pred < out_Ne_CMR$true ~ "sousestimation",
  TRUE ~ "correct"
)

ggplot(out_Ne_CMR, aes(x = estimation_type)) +
  geom_bar(fill = "steelblue") +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 5) +
  labs(title = "Répartition des types d'erreur parmi les outliers",
       x = "Type d’estimation",
       y = "Nombre de simulations") +
  theme_minimal(base_size = 14)


# Visualisation
ggplot(head(diffs_Ne_CMR, 10), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(title = "Différence de moyenne des stats (outliers vs non-outliers)",
       x = "Statistique résumante",
       y = "Différence de moyenne") +
  theme_minimal()

# Vraies valeurs
true_values_Ne_CMR <- learning_data_Ne_CMR$Harmonic_Ne

# Prédictions
predicted_Ne_CMR <- res_Ne_CMR$expectation

# RMSE
rmse_Ne_CMR <- sqrt(res_Ne_CMR$MSE)

# Biais
bias_Ne_CMR <- mean(predicted_Ne_CMR - true_values_Ne_CMR)

# R²
r2_Ne_CMR <- 1 - sum((true_values_Ne_CMR - predicted_Ne_CMR)^2) / sum((true_values_Ne_CMR - mean(true_values_Ne_CMR))^2)

# RMSE_relative
rmse_re_Ne_CMR <- mean((predicted_Ne_CMR - true_values_Ne_CMR)^2 / true_values_Ne_CMR^2)
sqrt(rmse_Ne_CMR)

# Biais_relatif
bias_re_Ne_CMR <- mean((predicted_Ne_CMR - true_values_Ne_CMR) / true_values_Ne_CMR)

coverage_Ne_CMR <- res_Ne_CMR$coverage

cat("Ne_CMR_only", "\n")
cat("Coverage    : ", coverage_Ne_CMR, "\n")
cat("RMSE        : ", rmse_Ne_CMR, "\n")
cat("Bias        : ", bias_Ne_CMR, "\n")
cat("R²          : ", r2_Ne_CMR, "\n")
cat("RMSE (rel)  : ", rmse_re_Ne_CMR, "\n")
cat("Bias (rel)  : ", bias_re_Ne_CMR, "\n")
```

```{r}

# ID des simulations outliers dans les deux modèles Ne
outliers_all_Ne <- out_Ne_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_gen_Ne <- out_Ne_gen %>% filter(is_outlier) %>% pull(simulation_id)

# Détection des groupes d'outliers
common_outliers_Ne <- intersect(outliers_all_Ne, outliers_gen_Ne)
only_all_Ne <- setdiff(outliers_all_Ne, outliers_gen_Ne)
only_gen_Ne <- setdiff(outliers_gen_Ne, outliers_all_Ne)

### 2. Extraire les données brutes ====

# 3 sous-jeux de données
stats_common_Ne <- learning_data_Ne_all %>%
  filter(simulation_id %in% common_outliers_Ne) %>%
  select(-simulation_id, -Harmonic_Ne)

stats_only_all_Ne <- learning_data_Ne_all %>%
  filter(simulation_id %in% only_all_Ne) %>%
  select(-simulation_id, -Harmonic_Ne)

stats_only_gen_Ne <- learning_data_Ne_gen %>%
  filter(simulation_id %in% only_gen_Ne) %>%
  select(-simulation_id, -Harmonic_Ne)

### 3. Identifier les outliers statistiques (z-score) ====

identify_stat_outliers <- function(df) {
  df_z <- scale(df)
  outlier_flags <- abs(df_z) > 2
  as.data.frame(outlier_flags)
}

flags_common_Ne <- identify_stat_outliers(stats_common_Ne)
flags_all_Ne <- identify_stat_outliers(stats_only_all_Ne)
flags_gen_Ne <- identify_stat_outliers(stats_only_gen_Ne)

### 4. Comptage des outliers par statistique ====

count_outliers <- function(flags_df, groupe) {
  tibble(
    stat = colnames(flags_df),
    count = colSums(flags_df, na.rm = TRUE),
    groupe = groupe
  )
}

count_common_Ne <- count_outliers(flags_common_Ne, "common")
count_all_Ne <- count_outliers(flags_all_Ne, "all_stats")
count_gen_Ne <- count_outliers(flags_gen_Ne, "genetic_only")

# Fusion des comptages
outlier_counts_Ne <- bind_rows(count_common_Ne, count_all_Ne, count_gen_Ne)

# Sélection des 5 stats les plus souvent aberrantes
top_stats_Ne <- outlier_counts_Ne %>%
  group_by(stat) %>%
  summarise(total = sum(count)) %>%
  arrange(desc(total)) %>%
  slice_head(n = 20) %>%
  pull(stat)

# Données filtrées pour le barplot
plot_data_Ne <- outlier_counts_Ne %>%
  filter(stat %in% top_stats_Ne)

### 5. Barplot empilé ====

ggplot(plot_data_Ne, aes(x = stat, y = count, fill = groupe)) +
  geom_col(position = "stack") +
  labs(
    title = "Statistiques influentes sur les outliers pour Ne",
    x = "Statistique résumante",
    y = "Nombre de simulations outliers",
    fill = "Origine des outliers"
  ) +
  scale_fill_manual(values = c("common" = "orange", "all_stats" = "skyblue", "genetic_only" = "salmon")) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r prediction vs true value - Ne_N_ratio}

# Create a dataframe with the real values and prediction 
prediction_df_Ne_N <- data.frame(
  Ne_N_ratio = learning_data_Ne_N$Ne_N_ratio,
  prediction = model_rf_Ne_N$model.rf$predictions
)

# Graphic representation
ggplot(prediction_df_Ne_N, aes(x = Ne_N_ratio, y = prediction)) +
  geom_point(shape = 1, alpha = 0.6) +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    x = "True value (Ne_N_ratio)",
    y = "Predicted value",
    title = "Prediction vs True values (abcrf)"
  ) +
  theme_minimal()

```

```{r}
# Fusion des prédictions
compare_preds <- data.frame(
  simulation_id = learning_data_N_all$simulation_id,
  true = learning_data_N_all$pop_size,
  pred_all = out_N_all$pred,
  pred_CMR = out_N_CMR$pred
)

# Identifier le type d'erreur (surestimation / sousestimation)
compare_preds <- compare_preds %>%
  mutate(
    type_all = case_when(
      pred_all > true ~ "surestimation",
      pred_all < true ~ "sousestimation",
      TRUE ~ "correct"
    ),
    type_CMR = case_when(
      pred_CMR > true ~ "surestimation",
      pred_CMR < true ~ "sousestimation",
      TRUE ~ "correct"
    )
  )

# Histogramme des erreurs dans out_N_all
out_N_all_with_type <- out_N_all %>%
  mutate(estimation_type = case_when(
    pred > true ~ "surestimation",
    pred < true ~ "sousestimation",
    TRUE ~ "correct"
  ))

ggplot(out_N_all_with_type %>% filter(is_outlier == TRUE), aes(x = true, fill = estimation_type)) +
  geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
  scale_x_log10() +
  labs(
    title = "All_data - Distribution des erreurs",
    x = "Valeur vraie de N (log10)",
    y = "Nombre de simulations",
    fill = "Type d'erreur"
  ) +
  theme_minimal()


# Histogramme des erreurs dans out_N_all
out_N_CMR_with_type <- out_N_CMR %>%
  mutate(estimation_type = case_when(
    pred > true ~ "surestimation",
    pred < true ~ "sousestimation",
    TRUE ~ "correct"
  ))

ggplot(out_N_CMR_with_type %>% filter(is_outlier == TRUE), aes(x = true, fill = estimation_type)) +
  geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
  scale_x_log10() +
  labs(
    title = "CMR_only - Distribution des erreurs",
    x = "Valeur vraie de N (log10)",
    y = "Nombre de simulations",
    fill = "Type d'erreur"
  ) +
  theme_minimal()

# Trouver les simulations avec surestimation dans les deux modèles
common_surestim <- compare_preds %>%
  filter(type_all == "surestimation" & type_CMR == "surestimation")

# Liste si besoin
common_surestim_ids <- common_surestim$simulation_id
nrow(common_surestim)  # combien partagées

# Comparaison des prédictions entre les deux modèles
compare_long <- compare_preds %>%
  select(simulation_id, true, pred_all, pred_CMR) %>%
  pivot_longer(cols = starts_with("pred_"), 
               names_to = "modele", values_to = "prediction") %>%
  mutate(modele = ifelse(modele == "pred_all", "Toutes les stats", "Stats spécifiques"))

ggplot(compare_long, aes(x = true, y = prediction, color = modele)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(title = "Comparaison des valeurs prédites : all vs spe",
       x = "Valeur vraie (N)",
       y = "Valeur prédite",
       color = "Modèle") +
  theme_minimal()

# Distribution des types d'erreur selon la classe de N
out_N_all_with_type %>%
  filter(is_outlier) %>%
  mutate(N_class = cut(true, breaks = c(0, 200, 500, 1000, 5000, 10000))) %>%
  group_by(N_class, estimation_type) %>%
  summarise(n = n(), .groups = "drop") %>%
  group_by(N_class) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = N_class, y = prop, fill = estimation_type)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(y = "Proportion", x = "Classe de N", fill = "Erreur", 
       title = "Répartition des types d'erreur selon les classes de N") +
  theme_minimal()

# Comparaison des prédictions pour les petites valeurs de N
compare_preds %>%
  filter(true < 500) %>%
  ggplot(aes(x = pred_all, y = pred_CMR)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(x = "Prédiction (all_stats)", y = "Prédiction (CMR_only)",
       title = "Comparaison des prédictions pour N < 500") +
  theme_minimal()

# Comparaison des prédictions pour les petites valeurs de N
compare_preds %>%
  filter(true > 8000) %>%
  ggplot(aes(x = pred_all, y = pred_CMR)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(x = "Prédiction (all_stats)", y = "Prédiction (CMR_only)",
       title = "Comparaison des prédictions pour N > 8000") +
  theme_minimal()

```

```{r}

# Fusion des prédictions
compare_preds_Ne <- data.frame(
  simulation_id = learning_data_Ne_all$simulation_id,
  true = learning_data_Ne_all$Harmonic_Ne,
  pred_all = out_Ne_all$pred,
  pred_gen = out_Ne_gen$pred
)

# Identifier le type d'erreur (surestimation / sous-estimation)
compare_preds_Ne <- compare_preds_Ne %>%
  mutate(
    type_all = case_when(
      pred_all > true ~ "surestimation",
      pred_all < true ~ "sousestimation",
      TRUE ~ "correct"
    ),
    type_gen = case_when(
      pred_gen > true ~ "surestimation",
      pred_gen < true ~ "sousestimation",
      TRUE ~ "correct"
    )
  )

# Histogramme des erreurs dans out_Ne_all
out_Ne_all_with_type <- out_Ne_all %>%
  mutate(estimation_type = case_when(
    pred > true ~ "surestimation",
    pred < true ~ "sousestimation",
    TRUE ~ "correct"
  ))

ggplot(out_Ne_all_with_type %>% filter(is_outlier == TRUE), aes(x = true, fill = estimation_type)) +
  geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
  scale_x_log10() +
  labs(
    title = "All_data - Distribution des erreurs",
    x = "Valeur vraie de Ne (log10)",
    y = "Nombre de simulations",
    fill = "Type d'erreur"
  ) +
  theme_minimal()

out_Ne_gen_with_type <- out_Ne_gen %>%
  mutate(estimation_type = case_when(
    pred > true ~ "surestimation",
    pred < true ~ "sousestimation",
    TRUE ~ "correct"
  ))

ggplot(out_Ne_gen_with_type %>% filter(is_outlier == TRUE), aes(x = true, fill = estimation_type)) +
  geom_histogram(bins = 50, position = "identity", alpha = 0.6) +
  scale_x_log10() +
  labs(
    title = "Genetic_only - Distribution des erreurs",
    x = "Valeur vraie de Ne (log10)",
    y = "Nombre de simulations",
    fill = "Type d'erreur"
  ) +
  theme_minimal()

# Simulations avec surestimation dans les deux modèles
common_surestim_Ne <- compare_preds_Ne %>%
  filter(type_all == "surestimation" & type_gen == "surestimation")

common_surestim_Ne_ids <- common_surestim_Ne$simulation_id
nrow(common_surestim_Ne)  # combien partagées

# Comparaison des prédictions entre les deux modèles
compare_long_Ne <- compare_preds_Ne %>%
  select(simulation_id, true, pred_all, pred_gen) %>%
  pivot_longer(cols = starts_with("pred_"), 
               names_to = "modele", values_to = "prediction") %>%
  mutate(modele = ifelse(modele == "pred_all", "Toutes les stats", "Stats spécifiques"))

ggplot(compare_long_Ne, aes(x = true, y = prediction, color = modele)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(title = "Comparaison des valeurs prédites : all vs spe (Ne)",
       x = "Valeur vraie (Ne)",
       y = "Valeur prédite",
       color = "Modèle") +
  theme_minimal()

# Répartition des types d'erreur selon les classes de Ne
out_Ne_all_with_type %>%
  filter(is_outlier) %>%
  mutate(Ne_class = cut(true, breaks = c(0, 100, 300, 1000, 3000, 10000))) %>%
  group_by(Ne_class, estimation_type) %>%
  summarise(n = n(), .groups = "drop") %>%
  group_by(Ne_class) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(aes(x = Ne_class, y = prop, fill = estimation_type)) +
  geom_bar(stat = "identity", position = "fill") +
  labs(y = "Proportion", x = "Classe de Ne", fill = "Erreur", 
       title = "Répartition des types d'erreur selon les classes de Ne") +
  theme_minimal()

# Comparaison des prédictions pour les petites valeurs de Ne
compare_preds_Ne %>%
  filter(true < 500) %>%
  ggplot(aes(x = pred_all, y = pred_gen)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(x = "Prédiction (all_stats)", y = "Prédiction (genetic_only)",
       title = "Comparaison des prédictions pour Ne < 500") +
  theme_minimal()

compare_preds_Ne %>%
  filter(true > 8000) %>%
  ggplot(aes(x = pred_all, y = pred_gen)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(x = "Prédiction (all_stats)", y = "Prédiction (genetic_only)",
       title = "Comparaison des prédictions pour Ne > 8000") +
  theme_minimal()

```

```{r}
compare_Ne_N <- data.frame(
  simulation_id = learning_data_N_all$simulation_id,
  true_N = out_N_all$true,
  pred_N_all = out_N_all$pred,
  pred_N_CMR = out_N_CMR$pred,
  true_Ne = out_Ne_all$true,
  pred_Ne_all = out_Ne_all$pred,
  pred_Ne_gen = out_Ne_gen$pred
)

compare_Ne_N <- compare_Ne_N %>%
  mutate(
    error_N_all = pred_N_all - true_N,
    error_Ne_all = pred_Ne_all - true_Ne,
    rel_error_N_all = abs(pred_N_all - true_N) / true_N,
    rel_error_Ne_all = abs(pred_Ne_all - true_Ne) / true_Ne,
    ratio_true = true_Ne / true_N,
    ratio_pred = pred_Ne_all / pred_N_all
  )

compare_Ne_N_long <- compare_Ne_N %>%
  select(simulation_id, rel_error_N_all, rel_error_Ne_all) %>%
  pivot_longer(cols = -simulation_id, names_to = "param", values_to = "rel_error")

ggplot(compare_Ne_N_long, aes(x = rel_error, fill = param)) +
  geom_histogram(bins = 60, alpha = 0.5, position = "identity") +
  scale_x_log10() +
  labs(title = "Distribution des erreurs relatives : N vs Ne",
       x = "Erreur relative (log10)", y = "Nombre de simulations", fill = "Paramètre") +
  theme_minimal()

threshold <- 0.5

compare_Ne_N <- compare_Ne_N %>%
  mutate(
    outlier_N = rel_error_N_all > threshold,
    outlier_Ne = rel_error_Ne_all > threshold,
    outlier_joint = case_when(
      outlier_N & outlier_Ne ~ "N & Ne",
      outlier_N ~ "N only",
      outlier_Ne ~ "Ne only",
      TRUE ~ "None"
    )
)

ggplot(compare_Ne_N, aes(x = outlier_joint)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Répartition des simulations aberrantes",
       x = "Type d’outlier", y = "Nombre de simulations")

compare_preds_long <- compare_Ne_N %>%
  pivot_longer(cols = c(true_N, pred_N_all, true_Ne, pred_Ne_all),
               names_to = c("type", "param"),
               names_sep = "_",
               values_to = "value") %>%
  pivot_wider(names_from = type, values_from = value)

ggplot(compare_preds_long, aes(x = true, y = pred)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  facet_wrap(~param, scales = "free") +
  scale_x_log10() + scale_y_log10() +
  labs(title = "Comparaison des valeurs vraies vs prédites pour N et Ne",
       x = "Valeur vraie", y = "Valeur prédite") +
  theme_minimal()

ggplot(compare_Ne_N, aes(x = ratio_true, y = ratio_pred)) +
  geom_point(alpha = 0.4) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  scale_x_log10() + scale_y_log10() +
  labs(title = "Comparaison des ratios Ne/N (vrai vs prédit)",
       x = "Ratio vrai Ne/N", y = "Ratio prédit Ne/N") +
  theme_minimal()

```

```{r}
# Sélection des colonnes numériques communes dans les deux groupes
common_cols <- intersect(
  colnames(out_N_all %>% filter(is_outlier) %>% select(where(is.numeric))),
  colnames(out_N_all %>% filter(!is_outlier) %>% select(where(is.numeric)))
)

# Moyenne des stats chez les outliers
out_stats_N_all <- out_N_all %>%
  filter(is_outlier) %>%
  select(all_of(common_cols)) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

# Moyenne des stats chez les non-outliers
non_out_stats_N_all <- out_N_all %>%
  filter(!is_outlier) %>%
  select(all_of(common_cols)) %>%
  summarise(across(everything(), mean, na.rm = TRUE))

# Calcul des différences de moyenne
diffs_N_all <- t(out_stats_N_all - non_out_stats_N_all) %>%
  as.data.frame() %>%
  rename(diff = V1) %>%
  tibble::rownames_to_column("stat") %>%
  arrange(desc(abs(diff)))

# Visualisation des 10 stats les plus différentes
ggplot(head(diffs_N_all, 20), aes(x = reorder(stat, diff), y = diff)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(
    title = "Top 10 des stats résumantes les plus différentes entre outliers et non-outliers",
    x = "Statistique résumante",
    y = "Différence de moyenne (outliers - non-outliers)"
  ) +
  theme_minimal(base_size = 14)

```

```{r}
outliers_all <- out_N_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_CMR <- out_N_CMR %>% filter(is_outlier) %>% pull(simulation_id)

common_outliers <- intersect(outliers_all, outliers_CMR)   # communs
only_all        <- setdiff(outliers_all, outliers_CMR)     # spécifiques à all
only_CMR        <- setdiff(outliers_CMR, outliers_all)     # spécifiques à spe

length(common_outliers)  # combien en commun
length(only_all)         # spécifiques à all_stats
length(only_CMR)         # spécifiques à stats spécifiques

# Venn diagram
library(ggvenn)
ggvenn(
  list(all_stats = outliers_all, specific_stats = outliers_CMR),
  fill_color = c("skyblue", "salmon"),
  stroke_size = 0.5,
  text_size = 4
)

compare_outliers <- out_N_all %>%
  select(simulation_id, rel_error_all = rel_error) %>%
  inner_join(out_N_CMR %>% select(simulation_id, rel_error_CMR = rel_error),
             by = "simulation_id")

ggplot(compare_outliers, aes(x = rel_error_all, y = rel_error_CMR)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Comparaison des erreurs relatives : all vs specific",
       x = "Erreur relative (all_stats)",
       y = "Erreur relative (stats spécifiques)") +
  theme_minimal()

```

```{r}
outliers_all <- out_N_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_CMR <- out_N_CMR %>% filter(is_outlier) %>% pull(simulation_id)

common_outliers <- intersect(outliers_all, outliers_CMR)   # communs
only_all        <- setdiff(outliers_all, outliers_CMR)     # spécifiques à all
only_CMR       <- setdiff(outliers_CMR, outliers_all)     # spécifiques à spe

length(common_outliers)  # combien en commun
length(only_all)         # spécifiques à all_stats
length(only_CMR)         # spécifiques à stats spécifiques

# Venn diagram
library(ggvenn)
ggvenn(
  list(all_stats = outliers_all, specific_stats = outliers_CMR),
  fill_color = c("skyblue", "salmon"),
  stroke_size = 0.5,
  text_size = 4
)

compare_outliers <- out_N_all %>%
  select(simulation_id, rel_error_all = rel_error) %>%
  inner_join(out_N_CMR %>% select(simulation_id, rel_error_CMR = rel_error),
             by = "simulation_id")

ggplot(compare_outliers, aes(x = rel_error_all, y = rel_error_CMR)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Comparaison des erreurs relatives : all vs specific",
       x = "Erreur relative (all_stats)",
       y = "Erreur relative (stats spécifiques)") +
  theme_minimal()

# ID des outliers
outliers_all <- out_N_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_CMR <- out_N_CMR %>% filter(is_outlier) %>% pull(simulation_id)

# Groupes
common_outliers <- intersect(outliers_all, outliers_CMR)
only_all        <- setdiff(outliers_all, outliers_CMR)
only_CMR        <- setdiff(outliers_CMR, outliers_all)

# Données stats sans la variable cible
stats_all <- learning_data_N_all %>% select(-pop_size, -simulation_id)
stats_CMR <- learning_data_N_CMR %>% select(-pop_size, -simulation_id)

# Associer simulation_id à chaque groupe
common_stats <- learning_data_N_all %>%
  filter(simulation_id %in% common_outliers) %>%
  select(-pop_size)

only_all_stats <- learning_data_N_all %>%
  filter(simulation_id %in% only_all) %>%
  select(-pop_size)

only_spe_stats <- learning_data_N_CMR %>%
  filter(simulation_id %in% only_CMR) %>%
  select(-pop_size)

# Non-outliers pour comparaison
non_out_stats <- learning_data_N_all %>%
  filter(!(simulation_id %in% union(outliers_all, outliers_CMR))) %>%
  select(-pop_size)

# Moyennes par groupe
mean_common     <- common_stats     %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_only_all   <- only_all_stats   %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_only_CMR   <- only_spe_stats   %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_non_out    <- non_out_stats    %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))

# Fonction pour calculer les écarts
compare_to_nonout <- function(mean_out, mean_nonout, label) {
  # Identifier les colonnes communes
  common_cols <- intersect(colnames(mean_out), colnames(mean_nonout))
  
  # Réaligner les deux dataframes sur ces colonnes
  mean_out_aligned <- mean_out[, common_cols]
  mean_nonout_aligned <- mean_nonout[, common_cols]
  
  # Calcul des différences
  diffs <- t(mean_out_aligned - mean_nonout_aligned) %>% as.data.frame()
  diffs$stat <- rownames(diffs)
  diffs$group <- label
  colnames(diffs)[1] <- "diff"
  return(diffs)
}


# Appliquer
diff_common   <- compare_to_nonout(mean_common, mean_non_out, "communs")
diff_only_all <- compare_to_nonout(mean_only_all, mean_non_out, "all_only")
diff_only_CMR <- compare_to_nonout(mean_only_CMR, mean_non_out, "spe_only")

# Regrouper les résultats
diffs_all <- bind_rows(diff_common, diff_only_all, diff_only_CMR)

top_diffs <- diffs_all %>%
  mutate(abs_diff = abs(diff)) %>%
  group_by(stat) %>%
  summarise(total = sum(abs_diff)) %>%
  arrange(desc(total)) %>%
  slice_head(n = 60) %>%
  pull(stat)

ggplot(diffs_all %>% filter(stat %in% top_diffs), 
       aes(x = reorder(stat, diff), y = diff, fill = group)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top stats différentielles entre outliers et non-outliers",
       x = "Statistique résumante",
       y = "Différence de moyenne (vs non-outliers)",
       fill = "Groupe d'outliers") +
  scale_fill_manual(values = c("communs" = "orange", "all_only" = "skyblue", "spe_only" = "salmon")) +
  theme_minimal()

```

```{r}
# 1. Identifier les outliers
outliers_all_Ne <- out_Ne_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_gen_Ne <- out_Ne_gen %>% filter(is_outlier) %>% pull(simulation_id)

# 2. Catégoriser les simulations
common_outliers_Ne <- intersect(outliers_all_Ne, outliers_gen_Ne)   # communs
only_all_Ne        <- setdiff(outliers_all_Ne, outliers_gen_Ne)     # spécifiques all
only_gen_Ne        <- setdiff(outliers_gen_Ne, outliers_all_Ne)     # spécifiques spe

# 3. Taille des groupes
length(common_outliers_Ne)
length(only_all_Ne)
length(only_gen_Ne)

# 4. Diagramme de Venn
library(ggvenn)
ggvenn(
  list(all_stats = outliers_all_Ne, specific_stats = outliers_gen_Ne),
  fill_color = c("skyblue", "salmon"),
  stroke_size = 0.5,
  text_size = 4
)

# 5. Comparaison des erreurs relatives pour les mêmes simulations
compare_outliers_Ne <- out_Ne_all %>%
  select(simulation_id, rel_error_all = rel_error) %>%
  inner_join(out_Ne_gen %>% select(simulation_id, rel_error_gen = rel_error),
             by = "simulation_id")

ggplot(compare_outliers_Ne, aes(x = rel_error_all, y = rel_error_gen)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Comparaison des erreurs relatives : Ne - all vs specific",
       x = "Erreur relative (all_stats)",
       y = "Erreur relative (stats spécifiques)") +
  theme_minimal()

```

```{r}
# 1. Identifier les IDs outliers
outliers_all_Ne <- out_Ne_all %>% filter(is_outlier) %>% pull(simulation_id)
outliers_gen_Ne <- out_Ne_gen %>% filter(is_outlier) %>% pull(simulation_id)

# 2. Grouper les cas
common_outliers_Ne <- intersect(outliers_all_Ne, outliers_gen_Ne)
only_all_Ne        <- setdiff(outliers_all_Ne, outliers_gen_Ne)
only_gen_Ne        <- setdiff(outliers_gen_Ne, outliers_all_Ne)

# 3. Taille des groupes
length(common_outliers_Ne)
length(only_all_Ne)
length(only_gen_Ne)

# 4. Diagramme de Venn
library(ggvenn)
ggvenn(
  list(all_stats = outliers_all_Ne, specific_stats = outliers_gen_Ne),
  fill_color = c("skyblue", "salmon"),
  stroke_size = 0.5,
  text_size = 4
)

# 5. Comparaison des erreurs relatives
compare_outliers_Ne <- out_Ne_all %>%
  select(simulation_id, rel_error_all = rel_error) %>%
  inner_join(out_Ne_gen %>% select(simulation_id, rel_error_gen = rel_error),
             by = "simulation_id")

ggplot(compare_outliers_Ne, aes(x = rel_error_all, y = rel_error_gen)) +
  geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Comparaison des erreurs relatives (Ne) : all vs specific",
       x = "Erreur relative (all_stats)",
       y = "Erreur relative (stats spécifiques)") +
  theme_minimal()

# 6. Extraction des statistiques pour les groupes
common_stats_Ne <- learning_data_Ne_all %>%
  filter(simulation_id %in% common_outliers_Ne) %>%
  select(-Harmonic_Ne)

only_all_stats_Ne <- learning_data_Ne_all %>%
  filter(simulation_id %in% only_all_Ne) %>%
  select(-Harmonic_Ne)

only_gen_stats_Ne <- learning_data_Ne_gen %>%
  filter(simulation_id %in% only_gen_Ne) %>%
  select(-Harmonic_Ne)

non_out_stats_Ne <- learning_data_Ne_all %>%
  filter(!(simulation_id %in% union(outliers_all_Ne, outliers_gen_Ne))) %>%
  select(-Harmonic_Ne)

# 7. Moyennes
mean_common_Ne     <- common_stats_Ne %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_only_all_Ne   <- only_all_stats_Ne %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_only_gen_Ne   <- only_gen_stats_Ne %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))
mean_non_out_Ne    <- non_out_stats_Ne %>% summarise(across(where(is.numeric), mean, na.rm = TRUE))

# 8. Fonction de comparaison
compare_to_nonout <- function(mean_out, mean_nonout, label) {
  common_cols <- intersect(colnames(mean_out), colnames(mean_nonout))
  mean_out_aligned <- mean_out[, common_cols]
  mean_nonout_aligned <- mean_nonout[, common_cols]
  
  diffs <- t(mean_out_aligned - mean_nonout_aligned) %>% as.data.frame()
  diffs$stat <- rownames(diffs)
  diffs$group <- label
  colnames(diffs)[1] <- "diff"
  return(diffs)
}

# 9. Application
diff_common_Ne   <- compare_to_nonout(mean_common_Ne, mean_non_out_Ne, "communs")
diff_only_all_Ne <- compare_to_nonout(mean_only_all_Ne, mean_non_out_Ne, "all_only")
diff_only_gen_Ne <- compare_to_nonout(mean_only_gen_Ne, mean_non_out_Ne, "spe_only")

diffs_all_Ne <- bind_rows(diff_common_Ne, diff_only_all_Ne, diff_only_gen_Ne)

# 10. Graphique : top stats
top_diffs_Ne <- diffs_all_Ne %>%
  mutate(abs_diff = abs(diff)) %>%
  group_by(stat) %>%
  summarise(total = sum(abs_diff)) %>%
  arrange(desc(total)) %>%
  slice_head(n = 60) %>%
  pull(stat)

ggplot(diffs_all_Ne %>% filter(stat %in% top_diffs_Ne), 
       aes(x = reorder(stat, diff), y = diff, fill = group)) +
  geom_col(position = "dodge") +
  coord_flip() +
  labs(title = "Top stats différentielles (Ne) : outliers vs non-outliers",
       x = "Statistique résumante",
       y = "Différence de moyenne",
       fill = "Groupe d'outliers") +
  scale_fill_manual(values = c("communs" = "orange", "all_only" = "skyblue", "spe_only" = "salmon")) +
  theme_minimal()

```

```{r}
plot(log(learning_data_Ne_all$Harmonic_Ne), learning_data_Ne_all$MatchCount_mean)
```

```{r}
# Nouveau dataframe avec uniquement les outliers (points rouges)
outliers_N_all <- out_N_all %>% filter(is_outlier)

# Affichage rapide pour vérifier
head(outliers_N_all)

# Nombre d’outliers
n_outliers <- nrow(outliers_N_all)
print(paste("Nombre d'outliers : ", n_outliers))

outlier_ids <- out_N_all %>%
  filter(is_outlier) %>%
  pull(random_1)

outlier_rows <- learning_data_N_all %>%
  filter(random_1 %in% outlier_ids)

non_outlier_rows <- learning_data_N_all %>%
  filter(!random_1 %in% outlier_ids)

all_labeled <- bind_rows(
  outlier_rows %>% mutate(is_outlier = TRUE),
  non_outlier_rows %>% mutate(is_outlier = FALSE)
)

library(dplyr)
library(broom)

# Sélectionner uniquement les colonnes numériques (stats résumantes)
correlations <- all_labeled %>%
  select(where(is.numeric), is_outlier) %>%
  summarise(across(-is_outlier, ~ cor(.x, is_outlier, method = "pearson"))) %>%
  pivot_longer(cols = everything(), names_to = "stat", values_to = "correlation_with_outlier") %>%
  arrange(desc(abs(correlation_with_outlier)))

print(correlations)

library(ggplot2)

ggplot(head(correlations, 10), aes(x = reorder(stat, correlation_with_outlier), y = correlation_with_outlier)) +
  geom_bar(stat = "identity", fill = "#F8766D") +
  coord_flip() +
  labs(
    title = "Corrélation entre les stats résumantes et l'appartenance aux outliers",
    x = "Statistique résumante",
    y = "Corrélation avec outlier (TRUE/FALSE)"
  ) +
  theme_minimal()

```

```{r choix_ntree, echo=TRUE, message=FALSE}

library(abcrf)

# 1) Votre suite de valeurs de ntree à tester
ntree_vals <- seq(100, 1000, by = 100)

# 2) Pour chaque ntree, on construit le modèle et on prend
#    la MSE OOB finale dans model.rf$prediction.error
oob_errors <- sapply(ntree_vals, function(nt) {
  model_rf_N_all <- regAbcrf(
    as.formula(paste(target_param_N, "~ .")),
    data  = learning_data_N_all,
    ntree = nt
  )
  # on récupère la MSE OOB finale
  model_rf_N_all$model.rf$prediction.error
})

# 3) Tracer Erreur OOB vs Nombre d’arbres
plot(ntree_vals, oob_errors,
     type = "b", pch = 19,
     xlab = "Nombre d’arbres (ntree)",
     ylab = "MSE OOB finale",
     main = "Sélection du nombre d’arbres pour ABCRF")

# 4) Identifier et annoter le minimum
opt_nt <- ntree_vals[which.min(oob_errors)]
abline(v = opt_nt, col = "red", lty = 2)
legend("topright",
       legend = sprintf("min OOB à %d arbres", opt_nt),
       col = "red", lty = 2, bty = "n")

# 5) Imprimer la valeur choisie
cat(sprintf("MSE OOB minimale = %.4f → choix de %d arbres\n",
            min(oob_errors), opt_nt))

```

```{r}

# 2) Assemble tout dans un data.frame
library(tidyr)
library(dplyr)

metrics_df <- tibble(
  Model    = factor(rep(c("Éco seul","Gén seul","Combo"), times = 1),
                    levels = c("Éco seul","Gén seul","Combo")),
  Metric   = rep("R²", each = 3),
  Value    = c(
    r2_N_CMR,    r2_N_gen,    r2_N_all
  )
)

# 3) Barplot facetté avec ggplot2
library(ggplot2)

ggplot(metrics_df, aes(x = Model, y = Value, fill = Model)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = ifelse(Metric %in% c("Coverage","R²"),
                               sprintf("%.2f", Value),
                               sprintf("%.3f", Value))),
            vjust = -0.5, size = 3) +
  facet_wrap(~ Metric, scales = "free_y", ncol = 3) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(x = NULL, y = NULL,
       title = "Comparaison des indicateurs de performance\npour les trois modèles ABC-RF pour l'estimation de N") +
  theme_minimal(base_size = 12) +
  theme(
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 15, hjust = 1)
  )

```

```{r}

r2_df <- data.frame(
  Model = rep(c("Éco seul","Gén seul","Combo"), times = 2),
  Param = factor(rep(c("N","Ne"), each = 3),
                 levels = c("N","Ne")),
  R2    = c(r2_N_CMR,  r2_N_gen,  r2_N_all,
            r2_Ne_CMR, r2_Ne_gen, r2_Ne_all)
)

# 3) Barplot groupé
ggplot(r2_df, aes(x = Model, y = R2, fill = Param)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.2f", R2)),
            position = position_dodge(width = 0.8),
            vjust = -0.4, size = 3) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(
    x     = "Type de modèle",
    y     = expression(R^2),
    fill  = "Paramètre",
    title = "Comparaison des R² pour l’estimation de N et de Ne"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x    = element_text(angle = 15, hjust = 1),
    legend.position = "top"
  )
```

```{r}

rmse_df <- data.frame(
  Model = rep(c("Éco seul","Gén seul","Combo"), times = 2),
  Param = factor(rep(c("N","Ne"), each = 3),
                 levels = c("N","Ne")),
  RMSE_rel    = c(rmse_re_N_CMR,  rmse_re_N_gen,  rmse_re_N_all,
            rmse_re_Ne_CMR, rmse_re_Ne_gen, rmse_re_Ne_all)
)

# 3) Barplot groupé
ggplot(rmse_df, aes(x = Model, y = RMSE_rel, fill = Param)) +
  geom_col(position = position_dodge(width = 0.8), width = 0.7) +
  geom_text(aes(label = sprintf("%.2f", RMSE_rel)),
            position = position_dodge(width = 0.8),
            vjust = -0.4, size = 3) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  scale_fill_manual(values = c("steelblue", "tomato")) +
  labs(
    x     = "Type de modèle",
    y     = "Erreur relative",
    fill  = "Paramètre",
    title = "Comparaison des erreurs relatives pour l’estimation de N et de Ne"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x    = element_text(angle = 15, hjust = 1),
    legend.position = "top"
  )
```

```{r abcrf_repeats_compare, message=FALSE}
library(abcrf)

# Nombre de répétitions
nrep <- 10
ntree <- 50

# Pré‐allouer les data.frames de perfs
perf_all <- data.frame(rep = 1:nrep,
  rmse = NA, bias = NA, r2 = NA,
  rmse_rel = NA, bias_rel = NA, coverage = NA
)
perf_CMR <- perf_gen <- perf_all  # mêmes colonnes, même structure

# Boucle des répétitions
for(i in seq_len(nrep)){
  set.seed(100 + i)
  
  # --- modèle ALL ---
# Use "pop-size" as the target parameter. It will be used by the ABC-RF model to choose which resuming statistics are the most useful to predict N values.
# Choose the parameter that will be predicted and create a new dataframe with it in the first column and the resuming stats
target_param_N <- "pop_size"
learning_data_N_all<- bind_cols(y = params[[target_param_N]], stats_table) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_all <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_all, ntree = 50)
res_N_all <- predictOOB(model_rf_N_all, training = learning_data_N_all)

# on récupère d’abord les vecteurs
true_vals <- learning_data_N_all$pop_size
pred_vals <- res_N_all$expectation

# puis on peut construire PERF
PERF <- list(
  rmse_N_all      = sqrt(mean((pred_vals - true_vals)^2)),
  bias_N_all      = mean(pred_vals - true_vals),
  r2_N_all        = cor(pred_vals, true_vals)^2,
  rmse_rel_N_all  = sqrt(mean(((pred_vals - true_vals)/true_vals)^2)),
  bias_rel_N_all  = mean((pred_vals - true_vals)/true_vals),
  coverage_N_all  = res_N_all$coverage
)
  perf_all[i, names(PERF)] <- unlist(PERF)
}
  
  # --- modèle CMR ---
for(i in seq_len(nrep)){
  set.seed(100 + i)
target_param_N <- "pop_size"
learning_data_N_CMR<- bind_cols(y = params_N[[target_param_N]], stats_table_N) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_CMR <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_CMR, ntree = 50)
res_N_CMR <- predictOOB(model_rf_N_CMR, training = learning_data_N_CMR)

# on récupère d’abord les vecteurs
true_vals <- learning_data_N_CMR$pop_size
pred_vals <- res_N_CMR$expectation

# puis on peut construire PERF
PERF <- list(
  rmse_N_CMR      = sqrt(mean((pred_vals - true_vals)^2)),
  bias_N_CMR      = mean(pred_vals - true_vals),
  r2_N_CMR        = cor(pred_vals, true_vals)^2,
  rmse_rel_N_CMR  = sqrt(mean(((pred_vals - true_vals)/true_vals)^2)),
  bias_rel_N_CMR  = mean((pred_vals - true_vals)/true_vals),
  coverage_N_CMR  = res_N_CMR$coverage
)

  perf_CMR[i, names(PERF)] <- unlist(PERF)
}  
  # --- modèle GEN ---
for(i in seq_len(nrep)){
  set.seed(100 + i)
target_param_N <- "pop_size"
learning_data_N_gen<- bind_cols(y = params_Ne[[target_param_N]], stats_table_Ne) %>% rename(!!target_param_N := y)

# Regression ABCRF that will predict the values of a parameters from the data base formed by the simulations
model_rf_N_gen <- regAbcrf(as.formula(paste(target_param_N, "~ .")), data = learning_data_N_gen, ntree = 50)
res_N_gen <- predictOOB(model_rf_N_gen, training = learning_data_N_gen)

# on récupère d’abord les vecteurs
true_vals <- learning_data_N_gen$pop_size
pred_vals <- res_N_gen$expectation

# puis on peut construire PERF
PERF <- list(
  rmse_N_gen      = sqrt(mean((pred_vals - true_vals)^2)),
  bias_N_gen      = mean(pred_vals - true_vals),
  r2_N_gen        = cor(pred_vals, true_vals)^2,
  rmse_rel_N_gen  = sqrt(mean(((pred_vals - true_vals)/true_vals)^2)),
  bias_rel_N_gen  = mean((pred_vals - true_vals)/true_vals),
  coverage_N_gen  = res_N_gen$coverage
)

  perf_gen[i, names(PERF)] <- unlist(PERF)
}
# 3) T‐tests ALL vs CMR pour chaque métrique
tests <- lapply(names(perf_all)[-1], function(metric){
  t.res <- t.test(perf_all[[metric]], perf_CMR[[metric]])
  data.frame(
    metric = metric,
    p.value = t.res$p.value,
    mean_all = mean(perf_all[[metric]]),
    mean_CMR = mean(perf_CMR[[metric]])
  )
})
results <- do.call(rbind, tests)
print(results)

# 4) Optionnel : affichage rapide des résultats
print("Moyennes et écarts‐types :")
summary_df <- rbind(
  ALL = sapply(perf_all[-1], function(x) sprintf("%.3f ± %.3f", mean(x), sd(x))),
  CMR = sapply(perf_CMR[-1], function(x) sprintf("%.3f ± %.3f", mean(x), sd(x))),
  GEN = sapply(perf_gen[-1], function(x) sprintf("%.3f ± %.3f", mean(x), sd(x)))
)
print(summary_df)

```

